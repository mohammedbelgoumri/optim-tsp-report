\chapter{Approximate Solutions 1 (Heuristics)}

Despite them being exact, the algorithms we introduced so far are not always practical to use on a real world problem. This is due to their computational expense. No efficient exact algorithm known for the \TSP. In fact, under the assumption \(\classP\neq\classNP\), no such algorithm exists. A reasonable alternative to consider is finding an efficient \emph{approximate} algorithm.

Approximate methods sacrifice the exactness of the solution for efficiency. The extent to which we can tolerate loss of accuracy is however limited. A constant time solution is useless if it gives a sufficiently large error.

\section{Nearest Neighbor}


All approximate methods are based on a compromise between accuracy and time. Nearest Neighbor being one such method, it is built on one such compromise. It places a higher emphasis on runtime than accuracy. In deed, we will see that Nearest Neighbor is the fastest among the algorithms we will consider. Correspondingly, it also gives the most mediocre solutions.

\subsection{The Algorithm}

Nearest Neighbor is what is called a \emph{construction heuristic}, which means it gives a solution that is feasible by construction, as opposed to \emph{improvement heuristics} which start with a feasible solution and then improve it to get a (locally) optimal solution.

It is also a \emph{greedy} algorithm. Greedy algorithms are iterative algorithms that take the optimal action at each iteration. The choice of action is based only on the information available at the current state. No backtracking is performed. They are therefore seldom able to find optimal solutions.

Nearest Neighbor in particular explores one branch of the search tree traversed by Branch and Bound. It is in deed equivalent to the first iteration of Branch and Bound. What it does is simply take the shortest edge to a city that has not been visited yet until no such city remains. This is exactly what Algorithm~\ref{algo:nearest-neighbor} does.

\begin{algorithm}
    \SetKwFunction{nearestNeighbors}{nearestNeighbors}
    \caption{Nearest Neighbor}
    \label{algo:nearest-neighbor}
    % \DontPrintSemicolon
    \KwIn{instance} \comment{represented as an adjacency matrix}
    \Begin{
    path \(\gets\) [0] \;
    {unvisited} \(\gets\) {cities} \(-\) {[origin]}\;
    \While{unvisited \(\neq \emptyset\)}{
    last \(\gets\) path[-1]\;
    \ForAll{city in \nearestNeighbors{last}}{
    \If{city \(\notin\) unvisited}{
        break
    }
    path \(\gets\) path + [city]\;
    visited \(\gets\) visited - [city]
    }
    }
    \KwOut{{path}}
    }
\end{algorithm}

\subsection{Performance Analysis}

In order to judge the performance of Algorithm~\ref{algo:nearest-neighbor}, two factors must be considered: runtime complexity and solution quality.

On the first measure, Nearest Neighbor does exceedingly well. Not only is its runtime polynomial in the number of cities, the degree of the polynomial is remarkably low. In fact, it can be shown that the runtime of Nearest Neighbor is \(\Theta(n^2)\), \(n\) being the number of cities (which is impressive given the difficulty of \TSP). This is because the main while loop is executed \(n\) times, and each call to \texttt{nearestNeighbors} runs in \(\Theta(n)\) time.

On the second however, it is difficult to come up with an algorithm that gives worse solutions than Nearest Neighbor. It is, in a certain sense, the worst possible solver of \TSP{}. This assessment is made rigorous by Proposition~\ref{prop:nn-anti}.

\begin{proposition}[Anti optimality of Nearest Neighbor]\ \\
    \label{prop:nn-anti}    
    Let \(n\in\naturals\), NN be a Nearest Neighbor solver.
    \begin{enumerate}[label=(\roman*)]
        \item \(\forall r\in[1, +\infty[\), there exists an instance \(x\) of \TSP{} with  at least\(n\) cities such that \(\mu_x(\mathrm{NN}(x)) \ge r\cdot\mathrm{opt}(x)\) 
        \item There exists an instance \(x\) of \TSP{} with at least \(n\) cities such that NN gives the worst tour of \(x\).
    \end{enumerate}
\end{proposition}

\begin{proof}
    See~\cite{tsp-tour}.
\end{proof}

Worse still, (ii) of Proposition~\ref{prop:nn-anti} persists even when we restrict our interest to the metric \TSP{} (i.e. if we suppose the triangle inequality). This is not the case for almost any other heuristic.

Intuitively, this is because despite the optimality of the first few edges of the nearest neighbor tour, the later edges can be very costly and can't be avoided due to the lack of backtracking.



\section{2-OPT and 3-OPT}

\begin{algorithm}
    \caption{2--OPT}
    \label{algo:2-opt}

    \KwIn{instance \comment{An instance of TSP} }
    \KwIn{starting\_tour \comment{A tour of the instance}}
    \SetKwFunction{range}{range}
    \SetKwFunction{distance}{distance}
    \SetKwFunction{swap}{swap}

    \Begin{
        \Repeat{exit}{
            exit \(gets\) true\;
            \ForEach(\comment{For evry city in the tour}){i \(\in \llbracket 0, n-1\rrbracket\)}{
                \ForEach(\comment{For evry city non-adjacent to i in the tour}){j in \(\in \llbracket 0, n-1\rrbracket- \{i - 1, i, i + 1\}\)}{
                    \If(\comment{If the swap is benificial}){\(\distance{i, i + 1} + \distance{j, j + 1} < \distance{i, j} + \distance{j + 1, i + 1}\)}{
                        \swap{i, i+1, j, j+1}\comment{Swap the edges}\;
                        exit \(gets\) false \comment{Keep looping}\;
                    }
                    
                }
            }
        }
    }
\end{algorithm}